{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a468e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d05265e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3607b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename):\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "    elif filename.endswith('.parquet'):\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    #df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34546f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_january = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet')\n",
    "df_february = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1342cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3066766, 19) (2913955, 19)\n"
     ]
    }
   ],
   "source": [
    "print(df_january.shape, df_february.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee6754",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744770c2",
   "metadata": {},
   "source": [
    "## Punto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1876f95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_january.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5734e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_january.tpep_dropoff_datetime = pd.to_datetime(df_january.tpep_dropoff_datetime)\n",
    "df_january.tpep_pickup_datetime = pd.to_datetime(df_january.tpep_pickup_datetime)\n",
    "\n",
    "df_february.tpep_dropoff_datetime = pd.to_datetime(df_february.tpep_dropoff_datetime)\n",
    "df_february.tpep_pickup_datetime = pd.to_datetime(df_february.tpep_pickup_datetime)\n",
    "\n",
    "df_january['duration'] = df_january.tpep_dropoff_datetime - df_january.tpep_pickup_datetime\n",
    "df_january.duration = df_january.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "\n",
    "df_february['duration'] = df_february.tpep_dropoff_datetime - df_february.tpep_pickup_datetime\n",
    "df_february.duration = df_february.duration.apply(lambda td: td.total_seconds() / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac174e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.594351241920904"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POINT 2\n",
    "df_january.duration.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8acdf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation of trip duration in January: 42.594351241920904\n"
     ]
    }
   ],
   "source": [
    "# Calcula la desviación estándar de la duración de los viajes\n",
    "std_duration = df_january['duration'].std()\n",
    "print(f'Standard Deviation of trip duration in January: {std_duration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7a2d48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3009173, 20) (2855951, 20)\n"
     ]
    }
   ],
   "source": [
    "# POINT 3\n",
    "df_january_filtered = df_january[(df_january.duration >= 1) & (df_january.duration <= 60)]\n",
    "df_february_filtered = df_february[(df_february.duration >= 1) & (df_february.duration <= 60)]\n",
    "print(df_january_filtered.shape, df_february_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6485a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3066766, 20) (2913955, 20)\n"
     ]
    }
   ],
   "source": [
    "print(df_january.shape, df_february.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e4c5591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left: 98% of the records\n"
     ]
    }
   ],
   "source": [
    "print(f\"Left: {round(df_january_filtered.shape[0]/df_january.shape[0] * 100)}% of the records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2213715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_january = df_january[(df_january.duration >= 1) & (df_january.duration <= 60)]\n",
    "df_february = df_february[(df_february.duration >= 1) & (df_february.duration <= 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14b1e980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
       "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
       "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
       "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
       "       'total_amount', 'congestion_surcharge', 'airport_fee', 'duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_january.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb1604de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POINT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aff5a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "520ba8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of the matrix: 515\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Asegúrate de que las columnas PULocationID y DOLocationID sean de tipo str\n",
    "df_january['PULocationID'] = df_january['PULocationID'].astype(str)\n",
    "df_january['DOLocationID'] = df_january['DOLocationID'].astype(str)\n",
    "\n",
    "# Convertir el dataframe en una lista de diccionarios, usando solo PULocationID y DOLocationID\n",
    "dicts = df_january[['PULocationID', 'DOLocationID']].to_dict(orient='records')\n",
    "\n",
    "# Paso 2: Ajustar el DictVectorizer usando una representación dispersa\n",
    "dv = DictVectorizer(sparse=True)\n",
    "X_sparse = dv.fit_transform(dicts)\n",
    "\n",
    "# Paso 3: Obtener la dimensionalidad de la matriz de características\n",
    "print(f'Dimensionality of the matrix: {X_sparse.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3738a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training data: 7.649261027806873\n"
     ]
    }
   ],
   "source": [
    "# POINT 5\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Usar la duración de los viajes como la variable objetivo\n",
    "y = df_january['duration'].values\n",
    "X = dv.fit_transform(dicts)\n",
    "\n",
    "# Crear y entrenar el modelo de regresión lineal\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predecir las duraciones usando el modelo entrenado\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calcular el RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f'RMSE on training data: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6a6f360",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3009173, 2855951]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Calcular el RMSE\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m rmse_validation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE on training data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_validation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:438\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_squared_error\u001b[39m(\n\u001b[1;32m    379\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    380\u001b[0m ):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    442\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:94\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m        the dtype argument passed to check_array.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m     96\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    335\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3009173, 2855951]"
     ]
    }
   ],
   "source": [
    "# POINT 6\n",
    "# Paso 1: Asegúrate de que las columnas PULocationID y DOLocationID sean de tipo str\n",
    "df_february['PULocationID'] = df_february['PULocationID'].astype(str)\n",
    "df_february['DOLocationID'] = df_february['DOLocationID'].astype(str)\n",
    "\n",
    "dicts_february = df_february[['PULocationID', 'DOLocationID']].to_dict(orient='records')\n",
    "# Usar la duración de los viajes como la variable objetivo\n",
    "y_test = df_january['duration'].values\n",
    "X_test = dv.transform(dicts_february)\n",
    "\n",
    "\n",
    "# Predecir las duraciones usando el modelo entrenado\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular el RMSE\n",
    "rmse_validation = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f'RMSE on training data: {rmse_validation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8fbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
